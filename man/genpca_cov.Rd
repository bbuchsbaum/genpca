% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpca_cov.R
\name{genpca_cov}
\alias{genpca_cov}
\title{Generalized PCA on a covariance matrix}
\usage{
genpca_cov(
  C,
  R = NULL,
  ncomp = NULL,
  method = c("gmd", "geigen"),
  constraints_remedy = c("error", "ridge", "clip", "identity"),
  tol = 1e-08,
  verbose = FALSE
)
}
\arguments{
\item{C}{A p x p symmetric positive semi-definite covariance matrix.
Typically C = X'MX where X is the data matrix and M is a row metric.}

\item{R}{Variable-side constraint/metric. Can be:
\itemize{
  \item{NULL: Identity matrix (standard PCA on C)}
  \item{A numeric vector of length p: Interpreted as diagonal weights (must be non-negative)}
  \item{A p x p symmetric PSD matrix: General metric/smoothing/structure penalties}
}}

\item{ncomp}{Number of components to return. Default is all positive eigenvalues.}

\item{method}{Character string specifying the method. One of:
\itemize{
  \item{"gmd" (default): Allen et al.'s GMD approach via eigen decomposition of R^{1/2} C R^{1/2}}
  \item{"geigen": Generalized eigenvalue approach solving C v = lambda R v}
}}

\item{constraints_remedy}{How to handle slightly non-PSD inputs (for geigen method). One of:
\itemize{
  \item{"error": Stop with an error if constraints are not PSD}
  \item{"ridge": Add a small ridge to the diagonal to make PSD}
  \item{"clip": Clip negative eigenvalues to zero}
  \item{"identity": Replace with identity matrix}
}}

\item{tol}{Numerical tolerance for PSD checks and filtering small eigenvalues. Default 1e-8.}

\item{verbose}{Logical. If TRUE, print progress messages. Default FALSE.}
}
\value{
A list with components:
  \describe{
    \item{v}{p x k matrix of loadings (R-orthonormal eigenvectors)}
    \item{d}{Singular values (square root of eigenvalues lambda)}
    \item{lambda}{Eigenvalues (variances under the R-metric)}
    \item{k}{Number of components returned}
    \item{propv}{Proportion of variance explained by each component}
    \item{cumv}{Cumulative proportion of variance explained}
    \item{R_rank}{Rank of the constraint matrix R}
    \item{method}{The method used ("gmd" or "geigen")}
  }
}
\description{
Performs Generalized PCA directly on a covariance matrix C with a single 
variable-side constraint/metric R. Supports two methods: "gmd" (Allen et al.'s 
GMD approach, default) which matches the two-sided genpca exactly, and "geigen" 
(generalized eigenvalue approach) which solves C v = lambda R v.
}
\details{
\strong{Method "gmd" (default):}

This method implements Allen et al.'s GMD approach and exactly matches the 
two-sided genpca when C = X'MX. It computes the eigendecomposition of 
R^{1/2} C R^{1/2} and maps back with V = R^{-1/2} Z, ensuring V'RV = I.
The total variance is tr(CR) as in Allen's GPCA (Corollary 5).

\strong{Method "geigen":}

This method solves the generalized eigenproblem C v = lambda R v directly.
While mathematically valid, it solves a different optimization than Allen's 
GMD and will not, in general, match the two-sided genpca unless R = I or 
special commutation conditions hold.

For exact equivalence with genpca(X, M, A), use method="gmd" with C = X'MX and R = A.
}
\examples{
# Standard PCA on covariance (no constraint)
C <- cov(scale(iris[,1:4], center=TRUE, scale=FALSE))
fit0 <- genpca_cov(C, R=NULL, ncomp=3)
print(fit0$d[1:3])       # first 3 singular values
print(fit0$propv[1:3])   # variance explained by first 3 components

# Variable weights via a diagonal metric
w <- c(1, 1, 0.5, 2)  # emphasize Sepal.Width less, Petal.Width more
fitW <- genpca_cov(C, R = w, ncomp=3, method="gmd")
print(fitW$d[1:3])

# Compare GMD and generalized eigenvalue approaches
fit_gmd <- genpca_cov(C, R = w, ncomp=2, method="gmd")
fit_geigen <- genpca_cov(C, R = w, ncomp=2, method="geigen")
# These will generally differ unless R = I
print(paste("GMD singular values:", paste(round(fit_gmd$d, 3), collapse=", ")))
print(paste("GEigen singular values:", paste(round(fit_geigen$d, 3), collapse=", ")))

}
\references{
Allen, G. I., Grosenick, L., & Taylor, J. (2014).
A Generalized Least-Squares Matrix Decomposition. 
Journal of the American Statistical Association, 109(505), 145-159.
}
