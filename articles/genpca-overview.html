<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>genpca: Generalized PCA and Related Decompositions • genpca</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="genpca: Generalized PCA and Related Decompositions">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">genpca</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/genpca-overview.html">genpca: Generalized PCA and Related Decompositions</a></li>
    <li><a class="dropdown-item" href="../articles/gplssvd-reference.html">Generalized PLS-SVD: Explicit Whitening Reference</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>genpca: Generalized PCA and Related Decompositions</h1>
            
      

      <div class="d-none name"><code>genpca-overview.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Principal Component Analysis (PCA) is one of the most widely used
techniques in data analysis, providing a foundation for dimensionality
reduction, data visualization, and feature extraction. This vignette
introduces Generalized PCA (GPCA), a powerful extension that adapts PCA
to the natural geometry of your data. Through practical examples and
clear explanations, you’ll learn when and how to move beyond standard
PCA to unlock deeper insights from complex datasets.</p>
<div class="section level3">
<h3 id="why-generalized-pca">Why Generalized PCA?<a class="anchor" aria-label="anchor" href="#why-generalized-pca"></a>
</h3>
<p>Standard Principal Component Analysis (PCA) assumes that all
variables and observations are equally important and that Euclidean
distance captures the relevant notion of similarity. However, many
real-world applications violate these assumptions.</p>
<p>Consider survey data where each observation represents responses from
populations of vastly different sizes. Standard PCA would treat a
response representing 10,000 people identically to one representing 100
people, potentially distorting the true population-level patterns.
Similarly, when variables are measured with different precisions—perhaps
some through expensive high-accuracy instruments and others through
approximate methods—we need a framework that can incorporate these
measurement characteristics directly into the analysis.</p>
<p>The challenge extends beyond simple weighting. In spatial and
temporal data, variables often exhibit known correlation structures that
should inform the dimensionality reduction. A temperature measurement at
one location is naturally correlated with nearby locations, and ignoring
this structure can lead to misleading components. Furthermore, many
specialized domains such as functional data analysis and shape analysis
operate in inherently non-Euclidean spaces where standard PCA’s
assumptions break down entirely.</p>
<p>Generalized PCA (GPCA) addresses these limitations by extending the
PCA framework to incorporate row and column metric constraints that
encode prior knowledge about the data structure. This principled
approach allows us to perform dimensionality reduction that respects the
natural geometry of our problem domain.</p>
</div>
<div class="section level3">
<h3 id="what-is-generalized-pca">What is Generalized PCA?<a class="anchor" aria-label="anchor" href="#what-is-generalized-pca"></a>
</h3>
<p>The <code>genpca</code> package implements Generalized PCA and
related matrix decompositions for data observed in non‑Euclidean
inner‑product spaces. At its core, GPCA transforms the standard PCA
problem by introducing two symmetric positive semi‑definite (PSD)
matrices that fundamentally alter how we measure distances and angles in
our data space.</p>
<p>Given an <code>n × p</code> data matrix <code>X</code>, the row
metric <code>M</code> (an <code>n × n</code> matrix) defines the inner
product on the observation space. This metric induces the M‑norm
<code>||x||_M^2 = x^T M x</code> and determines how we measure distances
between observations through
<code>d_M(x, y)^2 = (x − y)^T M (x − y)</code>. When working with
weighted sampling, we might choose <code>M</code> to be a diagonal
matrix of observation weights. For data with known correlation structure
among observations, <code>M</code> might encode precision (inverse
covariance) relationships. In applications involving temporal or spatial
data, <code>M</code> could be a kernel matrix capturing smoothness
assumptions.</p>
<p>Analogously, the column metric <code>A</code> (a <code>p × p</code>
matrix) defines the inner product on the variable space, inducing
<code>||v||_A^2 = v^T A v</code> and the distance
<code>d_A(v, w)^2 = (v − w)^T A (v − w)</code>. This metric might encode
feature importance through diagonal weights, capture known relationships
between variables through covariance or precision matrices, or represent
graph‑based connectivity in network data. The key insight is that
correlations and similarities are encoded through these metrics as inner
products, while dissimilarities emerge naturally as the induced
distances.</p>
<p>When both <code>M = I_n</code> and <code>A = I_p</code> (identity
matrices), we recover standard PCA as a special case, confirming that
GPCA is a true generalization that encompasses the familiar while
enabling much more.</p>
</div>
<div class="section level3">
<h3 id="package-capabilities">Package Capabilities<a class="anchor" aria-label="anchor" href="#package-capabilities"></a>
</h3>
<p>The <code>genpca</code> package provides a comprehensive toolkit for
generalized matrix decompositions. At its heart is the
<code><a href="../reference/genpca.html">genpca()</a></code> function, which implements GPCA through multiple
computational backends tailored to different problem scales and
structures. For small-to-medium datasets where forming full matrices is
feasible, the <code>method = "eigen"</code> option provides direct
eigendecomposition with excellent numerical stability. When dealing with
large-scale or sparse data, the <code>method = "spectra"</code> backend
leverages matrix-free iterative solvers implemented in C++ for
exceptional scalability. For situations where only a few components are
needed from massive datasets, <code>method = "deflation"</code> extracts
components sequentially, minimizing memory footprint.</p>
<p>Beyond the core decomposition, the package handles the practical
challenges of working with metric constraints. It automatically
validates and, when necessary, repairs metric matrices to ensure
positive semi-definiteness, using sophisticated remediation strategies
that preserve as much of the original structure as possible. This
robustness means you can focus on the statistical modeling rather than
numerical details.</p>
<p>The package integrates seamlessly with the <code>multivarious</code>
ecosystem, providing a unified workflow for preprocessing, projection,
reconstruction, and transfer learning across different dimensionality
reduction methods. This integration extends to Generalized Partial Least
Squares through the <code><a href="../reference/genpls.html">genpls()</a></code> function, enabling canonical
correlation analysis and related two-block methods with full metric
support.</p>
<p>This vignette introduces GPCA concepts through practical examples and
demonstrates core usage patterns. For those interested in the
mathematical foundations, particularly the connections to PLS-SVD and
whitening transformations, the companion vignette “Generalized PLS-SVD:
Explicit Whitening Reference” provides detailed theoretical
derivations.</p>
</div>
</div>
<div class="section level2">
<h2 id="mathematical-formulation">Mathematical Formulation<a class="anchor" aria-label="anchor" href="#mathematical-formulation"></a>
</h2>
<p>With this conceptual foundation in place, let’s examine the
mathematical formulation that makes GPCA both theoretically sound and
computationally tractable.</p>
<div class="section level3">
<h3 id="the-generalized-pca-problem">The Generalized PCA Problem<a class="anchor" aria-label="anchor" href="#the-generalized-pca-problem"></a>
</h3>
<p>GPCA seeks a low‑rank approximation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
that minimizes reconstruction error in a metric‑weighted sense. Given
row metric
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
and column metric
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,
we find
rank‑<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
factors by minimizing:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false" form="prefix">∥</mo><mi>X</mi><mo>−</mo><mi>U</mi><mi>D</mi><msup><mi>V</mi><mi>⊤</mi></msup><msubsup><mo stretchy="false" form="postfix">∥</mo><mrow><mi>M</mi><mo>,</mo><mi>A</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mo>tr</mo><mspace width="-0.167em"></mspace><mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mo><mi>M</mi><mspace width="0.167em"></mspace><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>−</mo><mi>U</mi><mi>D</mi><msup><mi>V</mi><mi>⊤</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mspace width="0.167em"></mspace><mi>A</mi><mspace width="0.167em"></mspace><msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>−</mo><mi>U</mi><mi>D</mi><msup><mi>V</mi><mi>⊤</mi></msup><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mo><mi>.</mi></mrow><annotation encoding="application/x-tex">
\lVert X - U D V^\top \rVert_{M,A}^2
= \operatorname{tr}\!\Big( M\, (X - U D V^\top)\, A\, (X - U D V^\top)^\top \Big).
</annotation></semantics></math></p>
<p>This weighted Frobenius norm incorporates the metrics into the
distance calculation. The solution satisfies orthonormality constraints
in the respective metrics:</p>
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>U</mi><mi>⊤</mi></msup><mi>M</mi><mi>U</mi><mo>=</mo><msub><mi>I</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">U^\top M U = I_k</annotation></semantics></math>
(M‑orthonormal scores)</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mi>⊤</mi></msup><mi>A</mi><mi>V</mi><mo>=</mo><msub><mi>I</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">V^\top A V = I_k</annotation></semantics></math>
(A‑orthonormal loadings)</li>
</ul>
</div>
<div class="section level3">
<h3 id="geometric-interpretation">Geometric Interpretation<a class="anchor" aria-label="anchor" href="#geometric-interpretation"></a>
</h3>
<p>The metrics <code>M</code> and <code>A</code> define the inner
products—and thus norms and distances—in the observation and variable
spaces. This is how weighting, correlation structure, and smoothness are
incorporated in a principled way. For example, choosing <code>M</code>
proportional to a precision (inverse covariance) emphasizes directions
with higher effective information, while a diagonal <code>M</code>
implements row weighting. When <code>M = I</code> and
<code>A = I</code>, we recover ordinary Euclidean geometry and standard
PCA.</p>
</div>
</div>
<div class="section level2">
<h2 id="getting-started-standard-pca-as-gpca">Getting Started: Standard PCA as GPCA<a class="anchor" aria-label="anchor" href="#getting-started-standard-pca-as-gpca"></a>
</h2>
<p>Now that we understand the mathematical framework, let’s see GPCA in
action. We’ll begin with standard PCA to establish a baseline, then
progressively introduce metrics to demonstrate how they modify the
decomposition and provide deeper insights into the data structure.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="co"># Simulated data: 200 observations, 50 variables</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">200</span> <span class="op">*</span> <span class="fl">50</span><span class="op">)</span>, <span class="fl">200</span>, <span class="fl">50</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://bbuchsbaum.github.io/genpca/">genpca</a></span><span class="op">)</span></span>
<span><span class="co">#&gt; Registered S3 method overwritten by 'genpca':</span></span>
<span><span class="co">#&gt;   method                   from        </span></span>
<span><span class="co">#&gt;   transfer.cross_projector multivarious</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Attaching package: 'genpca'</span></span>
<span><span class="co">#&gt; The following object is masked from 'package:base':</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     truncate</span></span>
<span><span class="co"># Standard PCA via GPCA (identity metrics by default)</span></span>
<span><span class="va">fit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, preproc <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Extract key components</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Top 5 singular values:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Top 5 singular values:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">fit</span><span class="op">$</span><span class="va">sdev</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 21.28910 20.34509 20.05655 19.84090 19.39831</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\nExplained variance (first 5 components):\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Explained variance (first 5 components):</span></span>
<span><span class="va">var_explained</span> <span class="op">&lt;-</span> <span class="va">fit</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">fit</span><span class="op">$</span><span class="va">sdev</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">var_explained</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span> <span class="op">*</span> <span class="fl">100</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 22.22 20.30 19.72 19.30 18.45</span></span>
<span></span>
<span><span class="co"># Scores and components follow multivarious conventions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/scores.html" class="external-link">scores</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">)</span>      <span class="co"># n × k matrix</span></span>
<span><span class="co">#&gt; [1] 200   5</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/components.html" class="external-link">components</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">)</span>  <span class="co"># p × k matrix</span></span>
<span><span class="co">#&gt; [1] 50  5</span></span></code></pre></div>
<p>This recovers standard PCA since we used identity metrics
(<code>M = I</code>, <code>A = I</code>).</p>
</div>
<div class="section level2">
<h2 id="weighted-observations-row-metrics">Weighted Observations: Row Metrics<a class="anchor" aria-label="anchor" href="#weighted-observations-row-metrics"></a>
</h2>
<p>Having established the baseline with standard PCA, we can now explore
how row metrics fundamentally change the analysis. Consider a scenario
where observations have different importance—a common situation in
survey data where each row might represent responses from vastly
different population sizes.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://Matrix.R-forge.R-project.org" class="external-link">Matrix</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Simulate observation weights (e.g., population sizes)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span>
<span><span class="va">obs_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/GammaDist.html" class="external-link">rgamma</a></span><span class="op">(</span><span class="va">n</span>, shape <span class="op">=</span> <span class="fl">2</span>, scale <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span><span class="va">obs_weights</span> <span class="op">&lt;-</span> <span class="va">obs_weights</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">obs_weights</span><span class="op">)</span>  <span class="co"># normalize</span></span>
<span></span>
<span><span class="co"># Create diagonal row metric</span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/Diagonal.html" class="external-link">Diagonal</a></span><span class="op">(</span><span class="va">n</span>, x <span class="op">=</span> <span class="va">obs_weights</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># GPCA with row weighting</span></span>
<span><span class="va">fit_weighted</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X</span>, M <span class="op">=</span> <span class="va">M</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, preproc <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare with unweighted</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Singular values (unweighted vs weighted):\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Singular values (unweighted vs weighted):</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span>Standard <span class="op">=</span> <span class="va">fit</span><span class="op">$</span><span class="va">sdev</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span>, Weighted <span class="op">=</span> <span class="va">fit_weighted</span><span class="op">$</span><span class="va">sdev</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt;      Standard Weighted</span></span>
<span><span class="co">#&gt; [1,] 21.28910 22.88953</span></span>
<span><span class="co">#&gt; [2,] 20.34509 21.80008</span></span>
<span><span class="co">#&gt; [3,] 20.05655 21.36878</span></span>
<span><span class="co">#&gt; [4,] 19.84090 20.82550</span></span>
<span><span class="co">#&gt; [5,] 19.39831 19.85992</span></span>
<span></span>
<span><span class="co"># The weighted decomposition emphasizes high-weight observations</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="variable-dependencies-column-metrics">Variable Dependencies: Column Metrics<a class="anchor" aria-label="anchor" href="#variable-dependencies-column-metrics"></a>
</h2>
<p>While row metrics weight observations, column metrics encode
relationships between variables. This capability becomes crucial when
dealing with variables that have known correlation structures, such as
measurements taken at different spatial locations or time points where
proximity implies similarity.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Example: Variables with local correlation structure</span></span>
<span><span class="co"># Create a tridiagonal matrix that is guaranteed to be PSD</span></span>
<span><span class="co"># This represents local smoothness constraints</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://Matrix.R-forge.R-project.org" class="external-link">Matrix</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Method 1: Simple diagonal weights with small correlations</span></span>
<span><span class="co"># Create a correlation-like structure that's guaranteed PSD</span></span>
<span><span class="va">A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/Diagonal.html" class="external-link">Diagonal</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span>  <span class="co"># Start with identity</span></span>
<span><span class="co"># Add small positive correlations between adjacent variables</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">p</span><span class="op">-</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">A</span><span class="op">[</span><span class="va">i</span>, <span class="va">i</span><span class="op">+</span><span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0.2</span></span>
<span>  <span class="va">A</span><span class="op">[</span><span class="va">i</span><span class="op">+</span><span class="fl">1</span>, <span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0.2</span></span>
<span><span class="op">}</span></span>
<span><span class="co"># Ensure it's positive definite by adding a small ridge</span></span>
<span><span class="va">A</span> <span class="op">&lt;-</span> <span class="va">A</span> <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/Diagonal.html" class="external-link">Diagonal</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># GPCA with column metric</span></span>
<span><span class="va">fit_corr</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X</span>, A <span class="op">=</span> <span class="va">A</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, preproc <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># The decomposition now accounts for variable dependencies</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Effect of column metric on first component:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Effect of column metric on first component:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/components.html" class="external-link">components</a></span><span class="op">(</span><span class="va">fit</span><span class="op">)</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>, </span>
<span>     <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/components.html" class="external-link">components</a></span><span class="op">(</span><span class="va">fit_corr</span><span class="op">)</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Standard PCA Component"</span>, </span>
<span>     ylab <span class="op">=</span> <span class="st">"GPCA Component (with dependencies)"</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"How column metrics change components"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html" class="external-link">abline</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, lty <span class="op">=</span> <span class="fl">2</span>, col <span class="op">=</span> <span class="st">"gray"</span><span class="op">)</span></span></code></pre></div>
<p><img src="genpca-overview_files/figure-html/column-dependencies-1.png" width="700"></p>
</div>
<div class="section level2">
<h2 id="computational-methods">Computational Methods<a class="anchor" aria-label="anchor" href="#computational-methods"></a>
</h2>
<p>Understanding how metrics affect the decomposition is one aspect of
GPCA; implementing it efficiently at scale is another. The package
provides multiple computational backends, each optimized for different
scenarios. The <code>method</code> parameter lets you select the most
appropriate approach for your data size and computational
constraints:</p>
<div class="section level3">
<h3 id="method-selection-guide">Method Selection Guide<a class="anchor" aria-label="anchor" href="#method-selection-guide"></a>
</h3>
<table class="table">
<colgroup>
<col width="17%">
<col width="21%">
<col width="30%">
<col width="15%">
<col width="15%">
</colgroup>
<thead><tr class="header">
<th>Method</th>
<th>Best For</th>
<th>Memory Usage</th>
<th>Speed</th>
<th>Notes</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>eigen</code></td>
<td>Small-medium data (n,p &lt; 1000)</td>
<td>High</td>
<td>Fast for small data</td>
<td>Forms full matrices</td>
</tr>
<tr class="even">
<td><code>spectra</code></td>
<td>Large sparse data</td>
<td>Low</td>
<td>Fast iterations</td>
<td>Matrix-free, requires C++</td>
</tr>
<tr class="odd">
<td><code>deflation</code></td>
<td>Few components needed</td>
<td>Minimal</td>
<td>Moderate</td>
<td>Sequential extraction</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compare methods on same data</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/joshuaulrich/microbenchmark/" class="external-link">microbenchmark</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Small dataset for quick comparison</span></span>
<span><span class="va">X_small</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">100</span> <span class="op">*</span> <span class="fl">30</span><span class="op">)</span>, <span class="fl">100</span>, <span class="fl">30</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Time different methods (eval=FALSE for vignette, but instructive)</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html" class="external-link">microbenchmark</a></span><span class="op">(</span></span>
<span>    eigen <span class="op">=</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X_small</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, method <span class="op">=</span> <span class="st">"eigen"</span><span class="op">)</span>,</span>
<span>    spectra <span class="op">=</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X_small</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, method <span class="op">=</span> <span class="st">"spectra"</span><span class="op">)</span>,</span>
<span>    deflation <span class="op">=</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X_small</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, method <span class="op">=</span> <span class="st">"deflation"</span><span class="op">)</span>,</span>
<span>    times <span class="op">=</span> <span class="fl">10</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># For large sparse problems, spectra is preferred</span></span>
<span><span class="co"># Note: When using sparse matrices, skip preprocessing or convert to dense</span></span>
<span><span class="va">n_large</span> <span class="op">&lt;-</span> <span class="fl">500</span>  <span class="co"># Smaller for vignette compilation</span></span>
<span><span class="va">p_large</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">X_sparse</span> <span class="op">&lt;-</span> <span class="fu">Matrix</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/rsparsematrix.html" class="external-link">rsparsematrix</a></span><span class="op">(</span><span class="va">n_large</span>, <span class="va">p_large</span>, density <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Convert to regular matrix for preprocessing compatibility</span></span>
<span><span class="co"># In practice, you might use preproc = multivarious::pass() with sparse matrices</span></span>
<span><span class="va">X_dense</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">as.matrix</a></span><span class="op">(</span><span class="va">X_sparse</span><span class="op">)</span></span>
<span><span class="va">fit_large</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X_dense</span>, ncomp <span class="op">=</span> <span class="fl">10</span>, method <span class="op">=</span> <span class="st">"spectra"</span>, </span>
<span>                    preproc <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'as(&lt;ddiMatrix&gt;, "dgCMatrix")' is deprecated.</span></span>
<span><span class="co">#&gt; Use 'as(as(., "generalMatrix"), "CsparseMatrix")' instead.</span></span>
<span><span class="co">#&gt; See help("Deprecated") and help("Matrix-deprecated").</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="preprocessing-and-reconstruction">Preprocessing and Reconstruction<a class="anchor" aria-label="anchor" href="#preprocessing-and-reconstruction"></a>
</h2>
<p>With the computational machinery in place, we turn to a critical
aspect of any PCA analysis: preprocessing and the ability to reconstruct
data from the learned components. The package’s integration with
<code>multivarious</code> provides a sophisticated preprocessing
pipeline that ensures numerical stability and statistical validity.
Centering, in particular, is typically essential for meaningful PCA as
it ensures components capture variance rather than being dominated by
the mean structure.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Center columns (variables) before decomposition</span></span>
<span><span class="va">fit_c</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X</span>, ncomp <span class="op">=</span> <span class="fl">10</span>, preproc <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Examine reconstruction error vs number of components</span></span>
<span><span class="va">recon_errors</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="kw">function</span><span class="op">(</span><span class="va">k</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">Xhat</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/reconstruct.html">reconstruct</a></span><span class="op">(</span><span class="va">fit_c</span>, comp <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">k</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">X</span> <span class="op">-</span> <span class="va">Xhat</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="va">recon_errors</span>, type <span class="op">=</span> <span class="st">"b"</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Number of Components"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Mean Squared Reconstruction Error"</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Reconstruction Quality"</span><span class="op">)</span></span></code></pre></div>
<p><img src="genpca-overview_files/figure-html/preproc-recon-1.png" width="700"></p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="co"># Typical elbow at 3-4 components for random data</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Reconstruction error with 3 components:"</span>, <span class="va">recon_errors</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Reconstruction error with 3 components: 0.8937022</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Percent variance retained:"</span>, </span>
<span>    <span class="fl">100</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">recon_errors</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">var</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html" class="external-link">as.vector</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, <span class="st">"%\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Percent variance retained: 12.79811 %</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="out-of-sample-projection">Out-of-Sample Projection<a class="anchor" aria-label="anchor" href="#out-of-sample-projection"></a>
</h2>
<p>A key strength of GPCA lies in its ability to generalize beyond the
training data. The learned decomposition defines a projection operator
that can map new observations into the component space, making GPCA
suitable for prediction, validation, and transfer learning
applications.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Split data for demonstration</span></span>
<span><span class="va">n_train</span> <span class="op">&lt;-</span> <span class="fl">150</span></span>
<span><span class="va">X_train</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="va">n_train</span>, <span class="op">]</span></span>
<span><span class="va">X_test</span> <span class="op">&lt;-</span> <span class="va">X</span><span class="op">[</span><span class="op">(</span><span class="va">n_train</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># Fit on training data</span></span>
<span><span class="va">fit_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">X_train</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, preproc <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Project test data into learned space</span></span>
<span><span class="va">scores_test</span> <span class="op">&lt;-</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/project.html" class="external-link">project</a></span><span class="op">(</span><span class="va">fit_train</span>, <span class="va">X_test</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Verify dimensions of projected scores</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Test scores dimensions:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">scores_test</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Test scores dimensions: 50 5</span></span>
<span></span>
<span><span class="co"># For reconstruction error, evaluate on the training split</span></span>
<span><span class="va">X_train_recon</span> <span class="op">&lt;-</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/reconstruct.html" class="external-link">reconstruct</a></span><span class="op">(</span><span class="va">fit_train</span>, comp <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span></span>
<span><span class="va">train_error</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">X_train</span> <span class="op">-</span> <span class="va">X_train_recon</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Train reconstruction error (5 comps):"</span>, <span class="va">train_error</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Train reconstruction error (5 comps): 0.8184143</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="working-with-covariance-matrices">Working with Covariance Matrices<a class="anchor" aria-label="anchor" href="#working-with-covariance-matrices"></a>
</h2>
<p>In many practical scenarios, you may encounter situations where the
raw data matrix is unavailable or too large to handle directly, but you
have access to a pre-computed covariance matrix C = X’MX. This arises
naturally when working with privacy-protected data, distributed
computing systems, or when the covariance has been computed
incrementally from streaming data. The <code><a href="../reference/genpca_cov.html">genpca_cov()</a></code>
function elegantly handles these cases by performing GPCA directly on
the covariance matrix:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Example: Large dataset where we only store the covariance</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">789</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">1000</span>  <span class="co"># Many observations</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">50</span>    <span class="co"># Moderate number of variables</span></span>
<span><span class="va">X_large</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="va">p</span><span class="op">)</span>, <span class="va">n</span>, <span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Row weights (e.g., sample importance)</span></span>
<span><span class="va">M</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">0.5</span>, <span class="fl">1.5</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compute and store only the covariance</span></span>
<span><span class="va">C</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/t.html" class="external-link">t</a></span><span class="op">(</span><span class="va">X_large</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">M</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="va">X_large</span>  <span class="co"># C = X'MX</span></span>
<span></span>
<span><span class="co"># Column constraint</span></span>
<span><span class="va">A</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="va">p</span>, <span class="fl">0.8</span>, <span class="fl">1.2</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># GPCA on covariance (memory-efficient for large n)</span></span>
<span><span class="va">fit_cov</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca_cov.html">genpca_cov</a></span><span class="op">(</span><span class="va">C</span>, R <span class="op">=</span> <span class="va">A</span>, ncomp <span class="op">=</span> <span class="fl">10</span>, method <span class="op">=</span> <span class="st">"gmd"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># This is mathematically equivalent to:</span></span>
<span><span class="co"># genpca(X_large, M = M, A = A, ncomp = 10)</span></span>
<span><span class="co"># but doesn't require storing the large X matrix</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Singular values from covariance GPCA:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Singular values from covariance GPCA:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">fit_cov</span><span class="op">$</span><span class="va">d</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 38.69639 38.17478 37.65776 37.33684 37.22870</span></span></code></pre></div>
<p>The <code><a href="../reference/genpca_cov.html">genpca_cov()</a></code> function becomes particularly valuable
in several scenarios. When the data matrix X is too large to store in
memory—perhaps containing millions of observations—storing only the p ×
p covariance matrix provides enormous memory savings. This approach also
shines when you receive pre-computed covariance matrices from external
sources where the raw data cannot be shared due to privacy constraints.
Additionally, when you need to explore multiple analyses with different
column constraints on the same dataset, working with the covariance
matrix allows you to avoid repeatedly processing the raw data.</p>
</div>
<div class="section level2">
<h2 id="generalized-partial-least-squares">Generalized Partial Least Squares<a class="anchor" aria-label="anchor" href="#generalized-partial-least-squares"></a>
</h2>
<p>The principles of GPCA extend naturally to two-block methods, where
we seek to understand relationships between two sets of variables. The
package implements generalized versions of Partial Least Squares that
respect the metric structure of both data blocks, opening up powerful
possibilities for canonical correlation analysis and related
techniques.</p>
<div class="section level3">
<h3 id="canonical-correlation-with-metrics">Canonical Correlation with Metrics<a class="anchor" aria-label="anchor" href="#canonical-correlation-with-metrics"></a>
</h3>
<p>The package provides generalized PLS-SVD (canonical PLS) for finding
relationships between two data matrices while respecting their metric
structures:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="co"># Create correlated data blocks</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="co"># X block: genomic measurements</span></span>
<span><span class="va">X_pls</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="fl">50</span><span class="op">)</span>, <span class="va">n</span>, <span class="fl">50</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Y block: phenotypic outcomes with some correlation to X</span></span>
<span><span class="va">shared_signal</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="fl">3</span><span class="op">)</span>, <span class="va">n</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">Y_pls</span> <span class="op">&lt;-</span> <span class="va">shared_signal</span> <span class="op"><a href="https://rdrr.io/r/base/matmult.html" class="external-link">%*%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">3</span> <span class="op">*</span> <span class="fl">20</span><span class="op">)</span>, <span class="fl">3</span>, <span class="fl">20</span><span class="op">)</span> <span class="op">+</span> </span>
<span>         <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="fl">20</span><span class="op">)</span>, <span class="va">n</span>, <span class="fl">20</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Canonical PLS to find shared patterns</span></span>
<span><span class="va">fit_pls</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpls.html">genpls</a></span><span class="op">(</span><span class="va">X_pls</span>, <span class="va">Y_pls</span>, ncomp <span class="op">=</span> <span class="fl">3</span>,</span>
<span>                  preproc_x <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>                  preproc_y <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Examine PLS-SVD singular values and latent correlations</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"PLS-SVD singular values (operator):\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; PLS-SVD singular values (operator):</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">fit_pls</span><span class="op">$</span><span class="va">d</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 527.977 397.665 327.720</span></span>
<span></span>
<span><span class="co"># If X and Y are standardized, singular values scale with (n-1).</span></span>
<span><span class="co"># Divide by (n-1) to obtain singular values of the cross-correlation operator.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\nSingular values of cross-correlation (d / (n-1)):\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Singular values of cross-correlation (d / (n-1)):</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">fit_pls</span><span class="op">$</span><span class="va">d</span> <span class="op">/</span> <span class="op">(</span><span class="va">n</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 2.653 1.998 1.647</span></span>
<span></span>
<span><span class="co"># Correlations between paired latent variables (always in [0, 1])</span></span>
<span><span class="va">cc</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="fl">3</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">j</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cor</a></span><span class="op">(</span><span class="va">fit_pls</span><span class="op">$</span><span class="va">lx</span><span class="op">[</span>, <span class="va">j</span><span class="op">]</span>, <span class="va">fit_pls</span><span class="op">$</span><span class="va">ly</span><span class="op">[</span>, <span class="va">j</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\nLatent variable correlations (approx. 'canonical-like'):\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Latent variable correlations (approx. 'canonical-like'):</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="va">cc</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.461 0.403 0.453</span></span>
<span></span>
<span><span class="co"># The weights show which variables contribute to each canonical variate</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\nDimensions of weight matrices:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Dimensions of weight matrices:</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"X weights:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">fit_pls</span><span class="op">$</span><span class="va">vx</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; X weights: 50 3</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"Y weights:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">fit_pls</span><span class="op">$</span><span class="va">vy</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Y weights: 20 3</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="low-level-operator-interface">Low-Level Operator Interface<a class="anchor" aria-label="anchor" href="#low-level-operator-interface"></a>
</h3>
<p>For advanced users, <code><a href="../reference/gplssvd_op.html">gplssvd_op()</a></code> provides direct access
to the operator-level computations without materializing large
intermediate matrices:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Direct operator SVD (useful for very large problems)</span></span>
<span><span class="va">op_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/gplssvd_op.html">gplssvd_op</a></span><span class="op">(</span><span class="va">X_pls</span>, <span class="va">Y_pls</span>, k <span class="op">=</span> <span class="fl">3</span>,</span>
<span>                        center <span class="op">=</span> <span class="cn">TRUE</span>, scale <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Same singular values as high-level interface</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/all.equal-methods.html" class="external-link">all.equal</a></span><span class="op">(</span><span class="va">op_result</span><span class="op">$</span><span class="va">d</span>, <span class="va">fit_pls</span><span class="op">$</span><span class="va">d</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="practical-example-weighted-survey-data">Practical Example: Weighted Survey Data<a class="anchor" aria-label="anchor" href="#practical-example-weighted-survey-data"></a>
</h2>
<p>To illustrate the practical impact of generalized metrics, let’s work
through a realistic example from survey research. This scenario
demonstrates not just how to use GPCA, but why the generalized framework
provides insights that standard PCA would miss.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Simulate survey data where observations represent different population sizes</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">n_respondents</span> <span class="op">&lt;-</span> <span class="fl">300</span></span>
<span><span class="va">n_questions</span> <span class="op">&lt;-</span> <span class="fl">20</span></span>
<span></span>
<span><span class="co"># Survey responses (Likert scale 1-5)</span></span>
<span><span class="va">survey_data</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="va">n_respondents</span> <span class="op">*</span> <span class="va">n_questions</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>, </span>
<span>                     <span class="va">n_respondents</span>, <span class="va">n_questions</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Population weights (some respondents represent more people)</span></span>
<span><span class="co"># E.g., stratified sampling with different sampling rates</span></span>
<span><span class="va">pop_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">50</span><span class="op">)</span>,   <span class="co"># Urban areas (undersampled)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">100</span><span class="op">)</span>,   <span class="co"># Suburban (representative)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">150</span><span class="op">)</span>    <span class="co"># Rural (slightly undersampled)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Standard PCA (treats all respondents equally)</span></span>
<span><span class="va">pca_unweighted</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">survey_data</span>, ncomp <span class="op">=</span> <span class="fl">5</span>, </span>
<span>                         preproc <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Weighted GPCA (accounts for population representation)</span></span>
<span><span class="va">M_survey</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/Matrix/man/Diagonal.html" class="external-link">Diagonal</a></span><span class="op">(</span><span class="va">n_respondents</span>, x <span class="op">=</span> <span class="va">pop_weights</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html" class="external-link">mean</a></span><span class="op">(</span><span class="va">pop_weights</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">pca_weighted</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/genpca.html">genpca</a></span><span class="op">(</span><span class="va">survey_data</span>, M <span class="op">=</span> <span class="va">M_survey</span>, ncomp <span class="op">=</span> <span class="fl">5</span>,</span>
<span>                      preproc <span class="op">=</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/center.html" class="external-link">center</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Compare first principal component patterns</span></span>
<span><span class="va">pc1_unweighted</span> <span class="op">&lt;-</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/components.html" class="external-link">components</a></span><span class="op">(</span><span class="va">pca_unweighted</span><span class="op">)</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span></span>
<span><span class="va">pc1_weighted</span> <span class="op">&lt;-</span> <span class="fu">multivarious</span><span class="fu">::</span><span class="fu"><a href="https://bbuchsbaum.github.io/multivarious/reference/components.html" class="external-link">components</a></span><span class="op">(</span><span class="va">pca_weighted</span><span class="op">)</span><span class="op">[</span>,<span class="fl">1</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># The weighted analysis gives different importance to questions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">pc1_unweighted</span>, <span class="va">pc1_weighted</span>,</span>
<span>     xlab <span class="op">=</span> <span class="st">"Unweighted PC1 Components"</span>,</span>
<span>     ylab <span class="op">=</span> <span class="st">"Population-Weighted PC1 Components"</span>,</span>
<span>     main <span class="op">=</span> <span class="st">"Effect of Population Weighting on Principal Components"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html" class="external-link">abline</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, lty <span class="op">=</span> <span class="fl">2</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html" class="external-link">text</a></span><span class="op">(</span><span class="va">pc1_unweighted</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span>, <span class="va">pc1_weighted</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span>, </span>
<span>     labels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"Q"</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span>, pos <span class="op">=</span> <span class="fl">4</span>, cex <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span></span></code></pre></div>
<p><img src="genpca-overview_files/figure-html/practical-example-1.png" width="700"></p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"\nCorrelation between weighted and unweighted PC1 loadings:"</span>, </span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cor</a></span><span class="op">(</span><span class="va">pc1_unweighted</span>, <span class="va">pc1_weighted</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Correlation between weighted and unweighted PC1 loadings: 0.8749888</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html" class="external-link">cat</a></span><span class="op">(</span><span class="st">"This shows that population weighting can substantially change the analysis.\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; This shows that population weighting can substantially change the analysis.</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="best-practices-and-performance-tips">Best Practices and Performance Tips<a class="anchor" aria-label="anchor" href="#best-practices-and-performance-tips"></a>
</h2>
<p>When working with the <code>genpca</code> package, several practices
can dramatically improve both performance and numerical stability.
Matrix sparsity should be leveraged whenever possible—the package is
designed to work efficiently with sparse <code>Matrix</code> objects,
which can reduce memory usage by orders of magnitude for data with many
zero entries.</p>
<p>Choosing the right computational method is crucial for performance.
For small dense problems with fewer than 1000 dimensions in either
direction, the <code>eigen</code> method provides the most
straightforward and numerically stable approach. As your data scales up
or becomes sparse, switching to the <code>spectra</code> method unlocks
matrix-free computations that can handle datasets too large to fit in
memory. When you need only a handful of components from a massive
dataset, the <code>deflation</code> method extracts them sequentially,
trading some computational efficiency for minimal memory usage.</p>
<p>The package’s automatic constraint validation saves you from
numerical pitfalls. When you provide metric matrices that are nearly but
not quite positive semi-definite—a common occurrence with empirical
covariance matrices—the package automatically detects and repairs them
using principled methods that preserve as much structure as
possible.</p>
<p>Preprocessing deserves special attention. Unless you have a specific
statistical reason to avoid it, always center your data before applying
GPCA. Centering ensures that the principal components capture variance
rather than being dominated by the mean structure. The integration with
<code>multivarious</code> makes this straightforward through the
<code>preproc</code> parameter.</p>
<p>For very large datasets that exceed available memory, the matrix-free
<code>spectra</code> method becomes essential. This approach never forms
the full transformed matrices, instead computing matrix-vector products
on demand. Combined with sparse matrix representations, this enables
GPCA on datasets that would be impossible to analyze with traditional
methods.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li><p>Allen, G. I., Grosenick, L., &amp; Taylor, J. (2014). A
generalized least-square matrix decomposition. <em>Journal of the
American Statistical Association</em>, 109(505), 145-159.</p></li>
<li><p>Beaton, D., ADNI, et al. (2016). Generalized partial least
squares: A framework for simultaneously capturing common and individual
variation. <em>NeuroImage</em>, 141, 346-363.</p></li>
<li><p>De Leeuw, J. (2007). Derivatives of generalized eigen systems
with applications. UCLA Department of Statistics Papers.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="further-resources">Further Resources<a class="anchor" aria-label="anchor" href="#further-resources"></a>
</h2>
<p>This vignette has introduced the core concepts and practical usage of
Generalized PCA. For those ready to dive deeper, several resources are
available. The companion vignette “Generalized PLS-SVD: Explicit
Whitening Reference” provides detailed mathematical derivations and
connections to the broader literature on matrix decompositions. The
package documentation (<code><a href="../reference/genpca.html">?genpca</a></code>) offers complete function
references with additional examples. For questions, bug reports, or
contributions, visit the GitHub repository at <a href="https://github.com/bbuchsbaum/genpca" class="external-link uri">https://github.com/bbuchsbaum/genpca</a>.</p>
<p>The generalized framework opens up a rich space of possibilities for
data analysis. By encoding prior knowledge through metrics, GPCA
transforms dimensionality reduction from a purely data-driven exercise
into a principled integration of domain expertise with statistical
learning.</p>
<pre><code><span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: x86_64-pc-linux-gnu</span></span>
<span><span class="co">#&gt; Running under: Ubuntu 24.04.3 LTS</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 </span></span>
<span><span class="co">#&gt; LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt;  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       </span></span>
<span><span class="co">#&gt;  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   </span></span>
<span><span class="co">#&gt;  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          </span></span>
<span><span class="co">#&gt; [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: UTC</span></span>
<span><span class="co">#&gt; tzcode source: system (glibc)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt; [1] microbenchmark_1.5.0 Matrix_1.7-3         genpca_0.1.0        </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] GPArotation_2025.3-1 sass_0.4.10          future_1.67.0       </span></span>
<span><span class="co">#&gt;  [4] generics_0.1.4       shape_1.4.6.1        multivarious_0.2.0  </span></span>
<span><span class="co">#&gt;  [7] lattice_0.22-7       listenv_0.9.1        digest_0.6.37       </span></span>
<span><span class="co">#&gt; [10] magrittr_2.0.4       evaluate_1.0.5       grid_4.5.1          </span></span>
<span><span class="co">#&gt; [13] RColorBrewer_1.1-3   iterators_1.0.14     fastmap_1.2.0       </span></span>
<span><span class="co">#&gt; [16] foreach_1.5.2        glmnet_4.1-10        jsonlite_2.0.0      </span></span>
<span><span class="co">#&gt; [19] ggrepel_0.9.6        RSpectra_0.16-2      survival_3.8-3      </span></span>
<span><span class="co">#&gt; [22] scales_1.4.0         pls_2.8-5            codetools_0.2-20    </span></span>
<span><span class="co">#&gt; [25] textshaping_1.0.3    jquerylib_0.1.4      cli_3.6.5           </span></span>
<span><span class="co">#&gt; [28] rlang_1.1.6          chk_0.10.0           parallelly_1.45.1   </span></span>
<span><span class="co">#&gt; [31] future.apply_1.20.0  splines_4.5.1        cachem_1.1.0        </span></span>
<span><span class="co">#&gt; [34] yaml_2.3.10          FNN_1.1.4.1          tools_4.5.1         </span></span>
<span><span class="co">#&gt; [37] parallel_4.5.1       dplyr_1.1.4          corpcor_1.6.10      </span></span>
<span><span class="co">#&gt; [40] ggplot2_4.0.0        PRIMME_3.2-6         globals_0.18.0      </span></span>
<span><span class="co">#&gt; [43] rsvd_1.0.5           assertthat_0.2.1     vctrs_0.6.5         </span></span>
<span><span class="co">#&gt; [46] R6_2.6.1             lifecycle_1.0.4      fs_1.6.6            </span></span>
<span><span class="co">#&gt; [49] irlba_2.3.5.1        ragg_1.5.0           pkgconfig_2.0.3     </span></span>
<span><span class="co">#&gt; [52] desc_1.4.3           geigen_2.3           pkgdown_2.1.3       </span></span>
<span><span class="co">#&gt; [55] pillar_1.11.0        bslib_0.9.0          gtable_0.3.6        </span></span>
<span><span class="co">#&gt; [58] glue_1.8.0           Rcpp_1.1.0           systemfonts_1.2.3   </span></span>
<span><span class="co">#&gt; [61] xfun_0.53            tibble_3.3.0         tidyselect_1.2.1    </span></span>
<span><span class="co">#&gt; [64] svd_0.5.8            knitr_1.50           farver_2.1.2        </span></span>
<span><span class="co">#&gt; [67] htmltools_0.5.8.1    rmarkdown_2.29       compiler_4.5.1      </span></span>
<span><span class="co">#&gt; [70] S7_0.2.0</span></span></code></pre>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Brad Buchsbaum.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
